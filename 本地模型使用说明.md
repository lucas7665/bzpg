# 本地模型使用说明

## 概述

本项目已集成本地DeepSeek模型支持，通过Ollama运行本地大模型，无需依赖外部API服务。

## 前置条件

1. 安装Ollama
   ```bash
   # macOS
   brew install ollama
   
   # 或者访问 https://ollama.ai 下载安装包
   ```

2. 下载DeepSeek模型
   ```bash
   # 下载DeepSeek R1模型（推荐，已安装）
   ollama pull deepseek-r1:7b
   
   # 或者下载其他DeepSeek模型
   ollama pull deepseek-coder
   ollama pull deepseek-chat
   ```

3. 启动Ollama服务
   ```bash
   ollama serve
   ```

## 配置说明

### 1. 启用本地模型

在 `application.yml` 中修改配置：

```yaml
ai:
  # 本地模型配置（Ollama）
  local:
    enabled: true  # 设置为true启用本地模型
    use_mock_data: false
    api:
      url: http://localhost:11434/api/generate  # Ollama默认端口
      model: deepseek-r1:7b  # 本地模型名称（已安装）
      temperature: 0.2
      max_tokens: 3000
      timeout: 300000
    prompt:
      file: classpath:prompts/evaluation-prompt.txt
```

### 2. 配置参数说明

- `enabled`: 本地模型开关，设置为 `true` 启用本地模型
- `url`: Ollama API地址，默认为 `http://localhost:11434/api/generate`
- `model`: 本地模型名称，需要与Ollama中下载的模型名称一致
- `temperature`: 生成文本的随机性，范围0-1
- `max_tokens`: 最大生成token数
- `timeout`: 请求超时时间（毫秒）

## 使用方法

### 1. 启动应用

确保Ollama服务正在运行，然后启动Spring Boot应用：

```bash
# 在项目根目录执行
mvn spring-boot:run
```

### 2. 访问评估页面

访问 `http://localhost:9999/admin/evaluation` 进行标准评估。

### 3. 本地模型管理

#### 查看当前模型
```bash
curl http://localhost:9999/admin/evaluation/current-model
# 返回格式：{"code":0,"data":"local:qwen2.5:7b-instruct"}
```

#### 查看支持的模型
```bash
curl http://localhost:9999/admin/evaluation/supported-models
# 包含云端模型和本地模型选项
```

#### 切换本地模型（运行时）
```bash
# 切换到 Qwen2.5-7B
curl -X POST http://localhost:9999/admin/evaluation/switch-model -d "modelName=local:qwen2.5:7b-instruct"

# 切换到 DeepSeek-R1
curl -X POST http://localhost:9999/admin/evaluation/switch-model -d "modelName=local:deepseek-r1:7b"
```

#### 切换到云端模型
```bash
# 切换到 Kimi
curl -X POST http://localhost:9999/admin/evaluation/switch-model -d "modelName=kimi"

# 切换到 DeepSeek
curl -X POST http://localhost:9999/admin/evaluation/switch-model -d "modelName=deepseek"
```

## 故障排除

### 1. 连接失败

如果出现连接失败错误，请检查：

- Ollama服务是否正在运行：`ollama list`
- 端口是否正确：默认11434
- 防火墙是否阻止连接

### 2. 模型不存在

如果出现模型不存在错误，请检查：

- 模型是否已下载：`ollama list`
- 模型名称是否与配置一致
- 重新下载模型：`ollama pull deepseek-coder`

### 3. 性能问题

本地模型可能比云端API慢，可以：

- 调整 `max_tokens` 参数减少生成内容长度
- 使用更小的模型
- 增加 `timeout` 参数

## 优势

1. **数据安全**: 数据不会发送到外部服务
2. **成本控制**: 无需支付API费用
3. **离线使用**: 无需网络连接
4. **定制化**: 可以微调本地模型

## 注意事项

1. 本地模型需要足够的计算资源（CPU/GPU）
2. 首次运行可能需要下载模型，需要一定时间
3. 建议在性能较好的机器上使用
4. 可以通过 `ollama ps` 查看模型运行状态
